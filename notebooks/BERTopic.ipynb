{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/garethsmith/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_DIR = \"../data/processed\"\n",
    "\n",
    "FILENAME = \"twitter_airline_sentiment_cleaned_emoji_urls_html_symbols@#_quotes_currency_whitespace\"\n",
    "\n",
    "EMBEDDING_MPNET = \"twitter_airline_sentiment_cleaned_emoji_urls_html_symbols@#_quotes_currency_whitespace_all-mpnet-base-v2.npy\"\n",
    "\n",
    "EMBEDDING_TWHINBERT = \"twitter_airline_sentiment_cleaned_emoji_urls_html_symbols@#_quotes_currency_whitespace_twhin-bert-base.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_mpnet = np.load(f\"{READ_DIR}/{EMBEDDING_MPNET}\")\n",
    "embeddings_twhinbert = np.load(f\"{READ_DIR}/{EMBEDDING_TWHINBERT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data\n",
    "df = pd.read_csv(f\"{READ_DIR}/{FILENAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for UMAP\n",
    "n_neighbors = 15\n",
    "n_components = 5\n",
    "min_dist = 0.1\n",
    "metric_umap = \"cosine\"\n",
    "random_state = 0\n",
    "\n",
    "# UMAP model\n",
    "reducer = UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    n_components=n_components,\n",
    "    min_dist=min_dist,\n",
    "    metric=metric_umap,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "# Parameters for HDBScan\n",
    "min_cluster_size = 15\n",
    "min_samples = 5\n",
    "metric_hdbscan = \"euclidean\"\n",
    "cluster_selection_method = \"eom\"\n",
    "\n",
    "# HDBScan model\n",
    "clusterer = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    metric=metric_hdbscan,\n",
    "    cluster_selection_method=cluster_selection_method,\n",
    "    prediction_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "# The lower and upper boundary of the range of n-values for different word n-grams\n",
    "# or char n-grams to be extracted:\n",
    "ngram_range = (1, 2)\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts:\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=tokenizer,\n",
    "    token_pattern=None,\n",
    "    ngram_range=ngram_range,\n",
    ")\n",
    "\n",
    "# Fit BERTopic model with customisation:\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer, umap_model=reducer, hdbscan_model=clusterer\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(list(df.clean_text), embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertmodel(ngram_range, umap_model, hdbscan_model):\n",
    "    # Convert a collection of text documents to a matrix of token counts:\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stopwords.words(\"english\"),\n",
    "        tokenizer=TweetTokenizer().tokenize,\n",
    "        token_pattern=None,\n",
    "        ngram_range=ngram_range,\n",
    "    )\n",
    "\n",
    "    # Fit BERTopic model with customisation:\n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer, umap_model=umap_model, hdbscan_model=hdbscan_model\n",
    "    )\n",
    "\n",
    "    return topic_model\n",
    "\n",
    "def get_topics_probs(model, docs, embeddings):\n",
    "    topics, probs = model.fit_transform(\n",
    "    docs, embeddings=embeddings)\n",
    "    return topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel = get_bertmodel(\n",
    "        ngram_range=(1,2),\n",
    "        umap_model=reducer, \n",
    "        hdbscan_model=clusterer)\n",
    "\n",
    "get_topics_probs(model=bertmodel,\n",
    "                 docs=list(df.clean_text),\n",
    "                 embeddings=EMBEDDING_MPNET,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
